{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 단어 임베딩 시각화 및 분석\n",
    "\n",
    "이 노트북에서는 앞서 학습한 GloVe 단어 임베딩을 시각화하고 분석합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 고차원 임베딩을 2D로 차원 축소하여 시각화하기\n",
    "- 단어 간 유사도와 의미적 관계 분석하기\n",
    "- 임베딩 공간의 특성 탐구하기\n",
    "- 워드 클러스터링 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# 시각화 설정\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# 한글 폰트 설정 (설정이 필요한 경우)\n",
    "try:\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "except:\n",
    "    print(\"한글 폰트 설정에 실패했습니다. 필요한 경우 시스템에 맞는 폰트를 설정해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 학습된 임베딩 불러오기\n",
    "\n",
    "이전 노트북에서 저장한 GloVe 임베딩을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# GloVe 임베딩 결과 불러오기\n",
    "try:\n",
    "    with open('glove_embeddings_results.pkl', 'rb') as f:\n",
    "        embeddings_results = pickle.load(f)\n",
    "    \n",
    "    center_embeddings = embeddings_results['center_embeddings']\n",
    "    context_embeddings = embeddings_results['context_embeddings']\n",
    "    combined_embeddings = embeddings_results['combined_embeddings']\n",
    "    word_to_id = embeddings_results['word_to_id']\n",
    "    id_to_word = embeddings_results['id_to_word']\n",
    "    embedding_dim = embeddings_results['embedding_dim']\n",
    "    losses = embeddings_results.get('losses', [])\n",
    "    \n",
    "    print(\"GloVe 임베딩 결과를 성공적으로 불러왔습니다.\")\n",
    "    print(f\"임베딩 크기: {combined_embeddings.shape}\")\n",
    "    print(f\"임베딩 차원: {embedding_dim}\")\n",
    "    print(f\"어휘 사전 크기: {len(word_to_id)} 단어\")\n",
    "except FileNotFoundError:\n",
    "    print(\"GloVe 임베딩 결과 파일을 찾을 수 없습니다. 먼저 '4_glove_loss_implementation.ipynb'를 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 임베딩 품질 평가\n",
    "\n",
    "학습 손실 곡선을 다시 시각화하고, 임베딩의 기본 통계량을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 학습 손실 곡선 시각화 (손실 데이터가 있는 경우)\n",
    "if losses:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(losses)+1), losses, marker='o')\n",
    "    plt.title('GloVe 학습 곡선', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 임베딩 통계량 확인\n",
    "norms = np.linalg.norm(combined_embeddings, axis=1)\n",
    "\n",
    "print(\"임베딩 통계:\")\n",
    "print(f\"평균 노름: {np.mean(norms):.4f}\")\n",
    "print(f\"표준편차 노름: {np.std(norms):.4f}\")\n",
    "print(f\"최소 노름: {np.min(norms):.4f}\")\n",
    "print(f\"최대 노름: {np.max(norms):.4f}\")\n",
    "\n",
    "# 노름 분포 히스토그램\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(norms, bins=30, alpha=0.7)\n",
    "plt.title('단어 벡터 노름 분포', fontsize=14)\n",
    "plt.xlabel('벡터 노름', fontsize=12)\n",
    "plt.ylabel('빈도', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 PCA를 사용한 차원 축소 및 시각화\n",
    "\n",
    "주성분 분석(PCA)을 사용하여 고차원 임베딩을 2D로 축소하고 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def reduce_dimensions_pca(embeddings, n_components=2):\n",
    "    \"\"\"PCA를 사용하여 임베딩 차원 축소\"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(embeddings)\n",
    "\n",
    "def visualize_embeddings_2d(embeddings_2d, words, title, figsize=(15, 12), max_words=100, annotate=True):\n",
    "    \"\"\"2D 임베딩 시각화\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # 표시할 단어 수 제한\n",
    "    n_words = min(len(words), max_words)\n",
    "    \n",
    "    # 산점도\n",
    "    plt.scatter(embeddings_2d[:n_words, 0], embeddings_2d[:n_words, 1], alpha=0.7)\n",
    "    \n",
    "    # 단어로 주석 달기\n",
    "    if annotate:\n",
    "        for i in range(n_words):\n",
    "            plt.annotate(words[i], (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=9)\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 자주 사용되는 단어 선택 (상위 100개)\n",
    "most_common_words = [word for word, _ in sorted(word_to_id.items(), key=lambda x: word_to_id[x[0]])[:100]]\n",
    "most_common_ids = [word_to_id[word] for word in most_common_words]\n",
    "\n",
    "# PCA로 차원 축소\n",
    "embeddings_pca = reduce_dimensions_pca(combined_embeddings[:100])\n",
    "\n",
    "# 시각화\n",
    "visualize_embeddings_2d(embeddings_pca, most_common_words, \"PCA로 축소한 단어 임베딩 (상위 100개 단어)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 t-SNE를 사용한 차원 축소 및 시각화\n",
    "\n",
    "t-SNE는 고차원 데이터의 비선형 구조를 더 잘 보존하는 차원 축소 기법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def reduce_dimensions_tsne(embeddings, n_components=2, perplexity=30, n_iter=1000):\n",
    "    \"\"\"t-SNE를 사용하여 임베딩 차원 축소\"\"\"\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, n_iter=n_iter, random_state=42)\n",
    "    return tsne.fit_transform(embeddings)\n",
    "\n",
    "# t-SNE로 차원 축소 (시간이 오래 걸릴 수 있음)\n",
    "print(\"t-SNE 차원 축소 중... (시간이 다소 걸릴 수 있습니다)\")\n",
    "embeddings_tsne = reduce_dimensions_tsne(combined_embeddings[:100])\n",
    "\n",
    "# 시각화\n",
    "visualize_embeddings_2d(embeddings_tsne, most_common_words, \"t-SNE로 축소한 단어 임베딩 (상위 100개 단어)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 단어 클러스터링\n",
    "\n",
    "임베딩 공간에서 비슷한 단어들을 클러스터링(군집화)합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def cluster_words(embeddings, n_clusters=5):\n",
    "    \"\"\"단어 임베딩 클러스터링\"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    return kmeans.fit_predict(embeddings)\n",
    "\n",
    "def visualize_word_clusters(embeddings_2d, words, clusters, title, figsize=(15, 12)):\n",
    "    \"\"\"클러스터링 결과 시각화\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # 색상 맵\n",
    "    colors = plt.cm.tab10.colors\n",
    "    \n",
    "    # 유니크한 클러스터 ID\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        # 해당 클러스터에 속하는 단어 인덱스\n",
    "        indices = np.where(clusters == cluster_id)[0]\n",
    "        \n",
    "        # 클러스터 데이터 포인트\n",
    "        cluster_points = embeddings_2d[indices]\n",
    "        \n",
    "        # 산점도\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1],\n",
    "                    color=colors[cluster_id % len(colors)],\n",
    "                    alpha=0.7, label=f'클러스터 {cluster_id}')\n",
    "        \n",
    "        # 단어로 주석 달기\n",
    "        for idx in indices:\n",
    "            plt.annotate(words[idx], (embeddings_2d[idx, 0], embeddings_2d[idx, 1]), fontsize=9)\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 가장 빈번한 단어 100개 선택 (또는 원하는 다른 단어 셋)\n",
    "selected_words = most_common_words[:100]\n",
    "selected_ids = [word_to_id[w] for w in selected_words]\n",
    "selected_embeddings = combined_embeddings[selected_ids]\n",
    "\n",
    "# 클러스터링 수행\n",
    "n_clusters = 5  # 클러스터 수 (필요에 따라 조정)\n",
    "clusters = cluster_words(selected_embeddings, n_clusters)\n",
    "\n",
    "# 클러스터링 결과 시각화 (앞서 계산한 t-SNE 결과 활용)\n",
    "visualize_word_clusters(embeddings_tsne, selected_words, clusters, \n",
    "                       f\"단어 임베딩 클러스터링 (k={n_clusters})\")\n",
    "\n",
    "# 클러스터별 단어 출력\n",
    "print(f\"\\n클러스터링 결과 (k={n_clusters}):\")\n",
    "for cluster_id in range(n_clusters):\n",
    "    indices = np.where(clusters == cluster_id)[0]\n",
    "    cluster_words = [selected_words[idx] for idx in indices]\n",
    "    print(f\"클러스터 {cluster_id}: {', '.join(cluster_words[:10])}\" + \n",
    "          (\"...\" if len(cluster_words) > 10 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 단어 간 유사도 분석\n",
    "\n",
    "특정 단어와 가장 유사한 단어들을 찾아봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def find_similar_words(word, embeddings, word_to_id, id_to_word, top_n=5):\n",
    "    \"\"\"특정 단어와 가장 유사한 단어들을 찾는 함수\"\"\"\n",
    "    if word not in word_to_id:\n",
    "        print(f\"단어 '{word}'를 어휘 사전에서 찾을 수 없습니다.\")\n",
    "        return []\n",
    "    \n",
    "    word_id = word_to_id[word]\n",
    "    word_vec = embeddings[word_id]\n",
    "    \n",
    "    # 벡터 정규화\n",
    "    word_vec_norm = np.linalg.norm(word_vec)\n",
    "    if word_vec_norm > 0:\n",
    "        word_vec = word_vec / word_vec_norm\n",
    "    \n",
    "    # 모든 단어 벡터 정규화\n",
    "    embeddings_norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    embeddings_norm[embeddings_norm == 0] = 1  # 0으로 나누기 방지\n",
    "    normalized_embeddings = embeddings / embeddings_norm\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    similarities = np.dot(normalized_embeddings, word_vec)\n",
    "    \n",
    "    # 유사도가 높은 상위 단어 ID (자기 자신 제외)\n",
    "    most_similar_ids = []\n",
    "    sorted_ids = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    for idx in sorted_ids:\n",
    "        if idx != word_id:  # 자기 자신 제외\n",
    "            most_similar_ids.append(idx)\n",
    "            if len(most_similar_ids) >= top_n:\n",
    "                break\n",
    "    \n",
    "    # 결과 생성\n",
    "    result = []\n",
    "    for idx in most_similar_ids:\n",
    "        result.append((id_to_word[idx], similarities[idx]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 예시: 몇 가지 단어에 대해 유사 단어 탐색\n",
    "example_words = most_common_words[:5]  # 상위 5개 단어\n",
    "for word in example_words:\n",
    "    similar_words = find_similar_words(word, combined_embeddings, word_to_id, id_to_word, top_n=5)\n",
    "    print(f\"\\n'{word}'와 가장 유사한 단어들:\")\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"  {similar_word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 요약 및 결론\n",
    "\n",
    "이 노트북에서는 GloVe로 학습한 단어 임베딩을 시각화하고 분석했습니다. 주요 단계는 다음과 같습니다:\n",
    "\n",
    "1. 임베딩 품질 평가 및 기본 통계 확인\n",
    "2. PCA와 t-SNE를 사용한 차원 축소 및 시각화\n",
    "3. K-means를 사용한 단어 클러스터링\n",
    "4. 단어 간 유사도 분석\n",
    "\n",
    "분석 결과를 통해 다음과 같은 통찰을 얻을 수 있습니다:\n",
    "\n",
    "1. 의미적으로 유사한 단어들은 임베딩 공간에서 서로 가깝게 위치합니다.\n",
    "2. 단어 벡터는 단어 간의 의미적 관계를 포착하며, 이를 통해 유사도를 계산할 수 있습니다.\n",
    "3. 클러스터링을 통해 유사한 의미를 가진 단어 그룹을 자동으로 찾을 수 있습니다.\n",
    "\n",
    "GloVe 모델은 단어의 동시출현 통계를 기반으로 학습하므로, 충분한 양의 텍스트 데이터가 있을 때 더 좋은 품질의 임베딩을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습 문제\n",
    "\n",
    "1. 다양한 t-SNE 하이퍼파라미터(perplexity, n_iter 등)를 시도하여 최적의 시각화 결과를 찾아보세요.\n",
    "2. 다른 클러스터링 알고리즘(예: DBSCAN, 계층적 클러스터링)을 시도하고 결과를 비교해보세요.\n",
    "3. 특정 주제와 관련된 단어들의 임베딩 공간 내 분포를 분석하고 해석해보세요.\n",
    "4. 단어 벡터 연산을 통해 단어 간의 의미적 관계를 탐색해보세요 (예: '왕 - 남자 + 여자 = ?')."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}