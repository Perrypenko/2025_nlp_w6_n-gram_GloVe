{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. GloVe 손실 함수 구현\n",
    "\n",
    "이 노트북에서는 GloVe(Global Vectors for Word Representation)의 손실 함수를 구현하고 단어 임베딩을 학습합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- GloVe 모델의 수학적 원리 이해하기\n",
    "- GloVe 손실 함수 직접 구현하기\n",
    "- 경사 하강법을 사용하여 단어 임베딩 학습하기\n",
    "- 학습 과정 시각화 및 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import scipy.sparse as sparse\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "\n",
    "# 시각화 설정\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 이전 단계 데이터 불러오기\n",
    "\n",
    "이전 노트북에서 저장한 동시출현 행렬과 어휘 사전을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 전처리 결과 불러오기\n",
    "try:\n",
    "    with open('preprocessing_results.pkl', 'rb') as f:\n",
    "        preprocessing_results = pickle.load(f)\n",
    "    \n",
    "    word_to_id = preprocessing_results['word_to_id']\n",
    "    id_to_word = preprocessing_results['id_to_word']\n",
    "    \n",
    "    print(\"전처리 결과를 성공적으로 불러왔습니다.\")\n",
    "    print(f\"어휘 사전 크기: {len(word_to_id)} 단어\")\n",
    "except FileNotFoundError:\n",
    "    print(\"전처리 결과 파일을 찾을 수 없습니다. 먼저 '1_corpus_preprocessing.ipynb'를 실행해주세요.\")\n",
    "\n",
    "# GloVe 동시출현 행렬 불러오기\n",
    "try:\n",
    "    with open('glove_matrix_results.pkl', 'rb') as f:\n",
    "        glove_matrix_results = pickle.load(f)\n",
    "    \n",
    "    cooccurrence_matrix = glove_matrix_results['cooccurrence_matrix']\n",
    "    window_size = glove_matrix_results['window_size']\n",
    "    \n",
    "    print(\"GloVe 동시출현 행렬을 성공적으로 불러왔습니다.\")\n",
    "    print(f\"행렬 크기: {cooccurrence_matrix.shape}\")\n",
    "    print(f\"0이 아닌 원소 수: {cooccurrence_matrix.nnz}\")\n",
    "    print(f\"윈도우 크기: {window_size}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"GloVe 동시출현 행렬 파일을 찾을 수 없습니다. 먼저 '3_glove_matrix.ipynb'를 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 GloVe 모델의 수학적 원리\n",
    "\n",
    "GloVe 모델은 단어의 동시출현 통계를 활용하여 단어 임베딩을 학습합니다. 모델의 핵심 아이디어는 두 단어 벡터의 내적이 해당 단어 쌍의 동시출현 확률의 로그와 비례해야 한다는 것입니다.\n",
    "\n",
    "GloVe 손실 함수는 다음과 같이 정의됩니다:\n",
    "\n",
    "$$J = \\sum_{i,j=1}^{V} f(X_{ij}) \\cdot (w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij})^2$$\n",
    "\n",
    "여기서:\n",
    "- $X_{ij}$는 단어 $i$와 단어 $j$의 동시출현 빈도\n",
    "- $w_i$와 $\\tilde{w}_j$는 각각 단어 $i$의 중심 벡터와 단어 $j$의 문맥 벡터\n",
    "- $b_i$와 $\\tilde{b}_j$는 각각 단어 $i$와 단어 $j$의 편향 항\n",
    "- $f(X_{ij})$는 가중치 함수로, 너무 빈번하거나 희귀한 동시출현에 대한 영향을 조절합니다.\n",
    "\n",
    "가중치 함수 $f(X_{ij})$는 다음과 같이 정의됩니다:\n",
    "\n",
    "$$f(x) = \\begin{cases}\n",
    "(x/x_{\\max})^{\\alpha} & \\text{if } x < x_{\\max} \\\\\n",
    "1 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "여기서 $x_{\\max}$는 최대 동시출현 빈도의 기준값이고, $\\alpha$는 보통 0.75로 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 GloVe 모델 구현\n",
    "\n",
    "먼저 PyTorch로 GloVe 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GloVeModel(nn.Module):\n",
    "    \"\"\"GloVe 모델 구현\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: 어휘 사전 크기\n",
    "        embedding_dim: 임베딩 차원\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(GloVeModel, self).__init__()\n",
    "        \n",
    "        # 중심 단어 임베딩 행렬\n",
    "        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 문맥 단어 임베딩 행렬\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 중심 단어 편향\n",
    "        self.center_biases = nn.Embedding(vocab_size, 1)\n",
    "        # 문맥 단어 편향\n",
    "        self.context_biases = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"가중치 초기화 함수\"\"\"\n",
    "        # 모든 임베딩 행렬의 가중치를 -0.5 ~ 0.5 사이의 균등 분포로 초기화\n",
    "        nn.init.uniform_(self.center_embeddings.weight, -0.5, 0.5)\n",
    "        nn.init.uniform_(self.context_embeddings.weight, -0.5, 0.5)\n",
    "        \n",
    "        # 편향 항을 0으로 초기화\n",
    "        nn.init.zeros_(self.center_biases.weight)\n",
    "        nn.init.zeros_(self.context_biases.weight)\n",
    "    \n",
    "    def forward(self, center_word_idx, context_word_idx):\n",
    "        \"\"\"순전파 함수\n",
    "        \n",
    "        Args:\n",
    "            center_word_idx: 중심 단어 인덱스\n",
    "            context_word_idx: 문맥 단어 인덱스\n",
    "            \n",
    "        Returns:\n",
    "            예측값 (중심 벡터와 문맥 벡터의 내적 + 편향)\n",
    "        \"\"\"\n",
    "        # 임베딩 가져오기\n",
    "        center_embeds = self.center_embeddings(center_word_idx)  # [batch_size, embedding_dim]\n",
    "        context_embeds = self.context_embeddings(context_word_idx)  # [batch_size, embedding_dim]\n",
    "        center_biases = self.center_biases(center_word_idx).squeeze()  # [batch_size]\n",
    "        context_biases = self.context_biases(context_word_idx).squeeze()  # [batch_size]\n",
    "        \n",
    "        # 예측값 계산\n",
    "        dot_product = torch.sum(center_embeds * context_embeds, dim=1)  # [batch_size]\n",
    "        log_cooccurrence = dot_product + center_biases + context_biases  # [batch_size]\n",
    "        \n",
    "        return log_cooccurrence\n",
    "    \n",
    "    def get_center_embeddings(self):\n",
    "        \"\"\"학습된 중심 단어 임베딩 반환\"\"\"\n",
    "        return self.center_embeddings.weight.detach().cpu().numpy()\n",
    "    \n",
    "    def get_context_embeddings(self):\n",
    "        \"\"\"학습된 문맥 단어 임베딩 반환\"\"\"\n",
    "        return self.context_embeddings.weight.detach().cpu().numpy()\n",
    "    \n",
    "    def get_combined_embeddings(self):\n",
    "        \"\"\"중심 단어 임베딩과 문맥 단어 임베딩의 평균 반환\"\"\"\n",
    "        center = self.center_embeddings.weight.detach().cpu().numpy()\n",
    "        context = self.context_embeddings.weight.detach().cpu().numpy()\n",
    "        return (center + context) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 가중치 함수 구현\n",
    "\n",
    "GloVe 논문에서 제안한 가중치 함수를 구현합니다. 이 함수는 너무 빈번하거나 희귀한 동시출현에 대한 영향을 조절합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def weight_func(x, x_max=100.0, alpha=0.75):\n",
    "    \"\"\"GloVe에서 사용하는 가중치 함수\n",
    "    \n",
    "    Args:\n",
    "        x: 동시출현 빈도\n",
    "        x_max: 최대 동시출현 빈도 기준값\n",
    "        alpha: 가중치 함수의 지수 (보통 0.75)\n",
    "        \n",
    "    Returns:\n",
    "        가중치 값\n",
    "    \"\"\"\n",
    "    x = torch.clamp(x, min=0)  # 음수 방지\n",
    "    return torch.minimum(torch.pow(x / x_max, alpha), torch.ones_like(x))\n",
    "\n",
    "# 가중치 함수 시각화\n",
    "x = np.linspace(0, 200, 1000)\n",
    "weights = weight_func(torch.tensor(x)).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, weights)\n",
    "plt.axvline(x=100, color='r', linestyle='--', alpha=0.5)\n",
    "plt.title('GloVe 가중치 함수 $f(x)$', fontsize=14)\n",
    "plt.xlabel('동시출현 빈도 ($X_{ij}$)', fontsize=12)\n",
    "plt.ylabel('가중치 ($f(X_{ij})$)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 GloVe 데이터셋 및 데이터 로더 구현\n",
    "\n",
    "동시출현 행렬에서 비영(non-zero) 원소를 추출하여 학습 데이터셋을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GloVeDataset(Dataset):\n",
    "    \"\"\"GloVe 학습을 위한 데이터셋\n",
    "    \n",
    "    동시출현 행렬에서 비영(non-zero) 원소를 추출하여 데이터셋 구성\n",
    "    \n",
    "    Args:\n",
    "        cooccurrence_matrix: 희소 행렬 형태의 동시출현 행렬\n",
    "        device: 텐서를 저장할 장치 (CPU 또는 GPU)\n",
    "    \"\"\"\n",
    "    def __init__(self, cooccurrence_matrix, device='cpu'):\n",
    "        self.device = device\n",
    "        \n",
    "        # 희소 행렬에서 비영(non-zero) 원소 추출\n",
    "        self.i_indices, self.j_indices = cooccurrence_matrix.nonzero()\n",
    "        self.values = cooccurrence_matrix.data\n",
    "        \n",
    "        print(f\"데이터셋 크기: {len(self.values)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center_word_idx = self.i_indices[idx]\n",
    "        context_word_idx = self.j_indices[idx]\n",
    "        cooccurrence = self.values[idx]\n",
    "        \n",
    "        # 텐서로 변환\n",
    "        center_word_idx = torch.tensor(center_word_idx, dtype=torch.long).to(self.device)\n",
    "        context_word_idx = torch.tensor(context_word_idx, dtype=torch.long).to(self.device)\n",
    "        cooccurrence = torch.tensor(cooccurrence, dtype=torch.float).to(self.device)\n",
    "        log_cooccurrence = torch.log(cooccurrence + 1e-8)  # 수치 안정성을 위해 작은 값 추가\n",
    "        \n",
    "        return center_word_idx, context_word_idx, cooccurrence, log_cooccurrence\n",
    "\n",
    "# GloVe 데이터셋 생성\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = GloVeDataset(cooccurrence_matrix, device=device)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "batch_size = 1024\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 GloVe 손실 함수 구현\n",
    "\n",
    "GloVe 논문에서 제안한 손실 함수를 구현합니다. 손실 함수는 예측값과 실제 동시출현 빈도의 로그 사이의 가중 평균 제곱 오차입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def glove_loss(predicted, log_cooccurrence, cooccurrence, x_max=100.0, alpha=0.75):\n",
    "    \"\"\"GloVe 손실 함수\n",
    "    \n",
    "    Args:\n",
    "        predicted: 모델의 예측값 (w_i^T w_j + b_i + b_j)\n",
    "        log_cooccurrence: 실제 동시출현 빈도의 로그값 (log X_ij)\n",
    "        cooccurrence: 실제 동시출현 빈도 (X_ij)\n",
    "        x_max: 최대 동시출현 빈도 기준값\n",
    "        alpha: 가중치 함수의 지수\n",
    "        \n",
    "    Returns:\n",
    "        손실 값\n",
    "    \"\"\"\n",
    "    # 가중치 계산\n",
    "    weights = weight_func(cooccurrence, x_max, alpha)\n",
    "    \n",
    "    # 손실 계산: f(X_ij) * (w_i^T w_j + b_i + b_j - log X_ij)^2\n",
    "    squared_diff = torch.pow(predicted - log_cooccurrence, 2)\n",
    "    weighted_squared_diff = weights * squared_diff\n",
    "    \n",
    "    return torch.mean(weighted_squared_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 GloVe 모델 학습\n",
    "\n",
    "구현한 GloVe 모델을 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(word_to_id)\n",
    "embedding_dim = 50  # 임베딩 차원\n",
    "learning_rate = 0.05  # 학습률\n",
    "num_epochs = 30  # 학습 에폭 수\n",
    "x_max = 100.0  # 최대 동시출현 빈도 기준값\n",
    "alpha = 0.75  # 가중치 함수의 지수\n",
    "\n",
    "# 모델 초기화\n",
    "model = GloVeModel(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 손실 기록을 위한 리스트\n",
    "losses = []\n",
    "\n",
    "# 학습 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for center_word_idx, context_word_idx, cooccurrence, log_cooccurrence in data_loader:\n",
    "        # 순전파\n",
    "        predicted = model(center_word_idx, context_word_idx)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = glove_loss(predicted, log_cooccurrence, cooccurrence, x_max, alpha)\n",
    "        \n",
    "        # 역전파 및 옵티마이저 스텝\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 손실 누적\n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    \n",
    "    # 에폭당 평균 손실 계산\n",
    "    avg_epoch_loss = epoch_loss / batch_count\n",
    "    losses.append(avg_epoch_loss)\n",
    "    \n",
    "    # 학습 진행 상황 출력\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.6f}, Time: {elapsed_time:.2f}s\")\n",
    "\n",
    "# 학습 소요 시간 출력\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n학습 완료: 총 {num_epochs} 에폭, 소요 시간: {total_time:.2f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 학습 곡선 시각화\n",
    "\n",
    "학습 중 손실 변화를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 학습 곡선 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs+1), losses, marker='o')\n",
    "plt.title('GloVe 학습 곡선', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 학습된 임베딩 저장\n",
    "\n",
    "학습된 단어 임베딩을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 학습된 임베딩 가져오기\n",
    "center_embeddings = model.get_center_embeddings()\n",
    "context_embeddings = model.get_context_embeddings()\n",
    "combined_embeddings = model.get_combined_embeddings()\n",
    "\n",
    "# 임베딩 저장\n",
    "embeddings_results = {\n",
    "    'center_embeddings': center_embeddings,\n",
    "    'context_embeddings': context_embeddings,\n",
    "    'combined_embeddings': combined_embeddings,\n",
    "    'word_to_id': word_to_id,\n",
    "    'id_to_word': id_to_word,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'losses': losses\n",
    "}\n",
    "\n",
    "with open('glove_embeddings_results.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_results, f)\n",
    "\n",
    "print(\"GloVe 임베딩 결과가 'glove_embeddings_results.pkl' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 임베딩 분석\n",
    "\n",
    "학습된 임베딩의 기본적인 특성을 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 임베딩 기본 통계 출력\n",
    "print(f\"임베딩 행렬 크기: {combined_embeddings.shape}\")\n",
    "print(f\"임베딩 평균: {np.mean(combined_embeddings):.6f}\")\n",
    "print(f\"임베딩 표준편차: {np.std(combined_embeddings):.6f}\")\n",
    "print(f\"임베딩 최소값: {np.min(combined_embeddings):.6f}\")\n",
    "print(f\"임베딩 최대값: {np.max(combined_embeddings):.6f}\")\n",
    "\n",
    "# 단어 벡터의 노름(norm) 분포 시각화\n",
    "norms = np.linalg.norm(combined_embeddings, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(norms, bins=30, alpha=0.7)\n",
    "plt.title('단어 벡터 노름 분포', fontsize=14)\n",
    "plt.xlabel('벡터 노름', fontsize=12)\n",
    "plt.ylabel('빈도', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11 유사 단어 탐색\n",
    "\n",
    "특정 단어와 가장 유사한 단어들을 찾는 함수를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def find_similar_words(word, embeddings, word_to_id, id_to_word, top_n=5):\n",
    "    \"\"\"특정 단어와 가장 유사한 단어들을 찾는 함수\n",
    "    \n",
    "    Args:\n",
    "        word: 대상 단어\n",
    "        embeddings: 단어 임베 딩 행렬\n",
    "        word_to_id: 단어 -> 인덱스 사전\n",
    "        id_to_word: 인덱스 -> 단어 사전\n",
    "        top_n: 유사 단어 개수\n",
    "        \n",
    "    Returns:\n",
    "        유사 단어 리스트\n",
    "    \"\"\"\n",
    "    if word not in word_to_id:\n",
    "        print(f\"'{word}' 단어는 어휘 사전에 없습니다.\")\n",
    "        return []\n",
    "    \n",
    "    # 단어 인덱스 가져오기\n",
    "    word_idx = word_to_id[word]\n",
    "    \n",
    "    # 단어 벡터 가져오기\n",
    "    word_vector = embeddings[word_idx]\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    similarities = np.dot(embeddings, word_vector) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(word_vector))\n",
    "    \n",
    "    # 유사도 정렬\n",
    "    similar_indices = np.argsort(similarities)[::-1]\n",
    "    similar_words = [(id_to_word[idx], similarities[idx]) for idx in similar_indices if idx != word_idx]\n",
    "    \n",
    "    # 상위 N개 단어 반환\n",
    "    return similar_words[:top_n]\n",
    "\n",
    "# 유사 단어 탐색 예시\n",
    "word = 'king'\n",
    "similar_words = find_similar_words(word, combined_embeddings, word_to_id, id_to_word, top_n=10)\n",
    "\n",
    "print(f\"'{word}'와 가장 유사한 단어들:\")\n",
    "for similar_word, similarity in similar_words:\n",
    "    print(f\"{similar_word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12 결론\n",
    "\n",
    "이 노트북에서는 GloVe 손실 함수를 구현하고 단어 임베딩을 학습했습니다. 학습된 임베딩은 단어 간의 의미적 유사성을 잘 반영하고 있습니다. 다음 단계에서는 이 임베딩을 활용하여 다양한 자연어 처리 태스크를 수행할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}